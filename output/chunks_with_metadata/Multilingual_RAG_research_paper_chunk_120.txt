METADATA: {'document_id': 'Multilingual_RAG_research_paper', 'section_name': 'open repo (link1, link2). We tune the model for three epochs with a batch size of 32 and a learning rate equal to 1e-5 with a 0.001 weight decay. We use the cosine learning rate scheduler with a warmup ratio of 0.03. We conducted our experiments on a workstation with four Nvidia RTX A6000 48VRAM for approximately 14 GPU/h. Languages German, Spanish, Italian, Russian, Chinese, Japanese, Arabic, Russian, Hindi Total: 18k Table 9: Instances and languages used for conducting Translation-following', 'chunk_id': 120}

open repo (link1, link2). We tune the model for three epochs with a batch size of 32 and a learning rate equal to 1e-5 with a 0.001 weight decay. We use the cosine learning rate scheduler with a warmup ratio of 0.03. We conducted our experiments on a workstation with four Nvidia RTX A6000 48VRAM for approximately 14 GPU/h. Languages German, Spanish, Italian, Russian, Chinese, Japanese, Arabic, Russian, Hindi Total: 18k Table 9: Instances and languages used for conducting Translation-following